{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SVM_scripts as svm\n",
    "import sklearn as skl\n",
    "import skopt.space as sks\n",
    "from tqdm import tqdm\n",
    "\n",
    "qpu_access_time_50 = 29279.56e-6\n",
    "qpu_access_time_100 = 42793.56e-6\n",
    "qpu_access_time_200 = 69827.96e-6\n",
    "qpu_access_time_250 = 83335.56e-6\n",
    "qpu_access_time_500 = 150905.56e-6\n",
    "qpu_access_time_1000 = 286051.96e-6\n",
    "qpu_access_time_2000 = 556331.96e-6\n",
    "qpu_access_time_2500 = 691465.56e-6\n",
    "qpu_access_time_3000 = 826605.56e-6\n",
    "\n",
    "# The problem cannot be submitted because its estimated QPU access time exceeds the maximum of 1000000 microseconds for Advantage_system4.1. To resolve this issue, see the topic at https://docs.dwavesys.com/docs/latest/c_qpu_timing.html#keeping-within-the-runtime-limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice size:  44\n",
      "Slice size:  59\n",
      "Slice size:  88\n",
      "Slice size:  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = 'Polaris/pgp-broccatelli.parquet'\n",
    "# file_name = 'Polaris/bbb-martins.parquet'\n",
    "# file_name = 'Polaris/ncats-solubility.parquet'\n",
    "\n",
    "matrix, matrix_labels, names = svm.read_parquet(file_name)\n",
    "\n",
    "slice_sizes = [int(177/n) for n in [4,3,2,1]]\n",
    "N = 1\n",
    "limits = [34,26,18,12]\n",
    "fit_count = 0\n",
    "\n",
    "for i in tqdm(range(0,N)):\n",
    "    vectors, labels, test_vectors, test_labels = svm.prepare_data_sets(matrix, matrix_labels, train_percentage=50, positive_negative_ratio=None, max_train_size=None, min_train_size=None, seed=i, normalize_data=True, print_info=False)\n",
    "    default_gamma = 1 / (vectors.shape[1] * vectors.var())\n",
    "    # param_grid = {'base': [2,10], 'num_encoding': [1,2,3,4],'kernel': [n*default_gamma for n in [0.2,0.4]], 'penalty': [0,1]}\n",
    "    search_space = {\n",
    "        'base': sks.Categorical([2,10]),\n",
    "        'num_encoding': sks.Integer(1,4),\n",
    "        'kernel': sks.Real(0.05 * default_gamma, 2 * default_gamma, prior='log-uniform'),\n",
    "        'penalty': sks.Real(0,6),\n",
    "    }\n",
    "    # search_sizes = svm.estimate_search_space_size(svm.split_dwave_search_space(search_space, slice_size=None, max_qubo_dim=177), samples_per_real_interval=5)\n",
    "    # limit = round(sum([svm.samples_per_size(search_size) for search_size in search_sizes]))\n",
    "    inner_estimator = svm.qSVM_estimator(solver='SA', adjust_bias=False)\n",
    "    for s, slice_size in enumerate(slice_sizes):\n",
    "        print('Slice size: ', slice_size)\n",
    "        limit = limits[s]\n",
    "        estimator = svm.slice_estimator(estimator=inner_estimator, slice_size=slice_size, force_unbiased=False, adjust_outer_bias=True, seed=0)\n",
    "        fits = svm.count_hyperparameter_optimization_fits(estimator, search_space=search_space, vectors=vectors, labels=labels, folds=4, filter=True, limit=limit, mode='bayes', seed=0, print_info=True)\n",
    "        fit_count += fits\n",
    "\n",
    "fit_count1 = fit_count\n",
    "print(fit_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice size:  44\n",
      "Slice size:  59\n",
      "Slice size:  88\n",
      "Slice size:  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:38<00:00, 98.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# file_name = 'Polaris/pgp-broccatelli.parquet'\n",
    "file_name = 'Polaris/bbb-martins.parquet'\n",
    "# file_name = 'Polaris/ncats-solubility.parquet'\n",
    "\n",
    "matrix, matrix_labels, names = svm.read_parquet(file_name)\n",
    "\n",
    "slice_sizes = [int(177/n) for n in [4,3,2,1]]\n",
    "N = 1\n",
    "limits = [34,26,18,12]\n",
    "fit_count = 0\n",
    "\n",
    "for i in tqdm(range(0,N)):\n",
    "    vectors, labels, test_vectors, test_labels = svm.prepare_data_sets(matrix, matrix_labels, train_percentage=50, positive_negative_ratio=None, max_train_size=None, min_train_size=None, seed=i, normalize_data=True, print_info=False)\n",
    "    default_gamma = 1 / (vectors.shape[1] * vectors.var())\n",
    "    # param_grid = {'base': [2,10], 'num_encoding': [1,2,3,4],'kernel': [n*default_gamma for n in [0.2,0.4]], 'penalty': [0,1]}\n",
    "    search_space = {\n",
    "        'base': sks.Categorical([2,10]),\n",
    "        'num_encoding': sks.Integer(1,4),\n",
    "        'kernel': sks.Real(0.05 * default_gamma, 2 * default_gamma, prior='log-uniform'),\n",
    "        'penalty': sks.Real(0,6),\n",
    "    }\n",
    "    # search_sizes = svm.estimate_search_space_size(svm.split_dwave_search_space(search_space, slice_size=None, max_qubo_dim=177), samples_per_real_interval=5)\n",
    "    # limit = round(sum([svm.samples_per_size(search_size) for search_size in search_sizes]))\n",
    "    inner_estimator = svm.qSVM_estimator(solver='SA', adjust_bias=False)\n",
    "    for s, slice_size in enumerate(slice_sizes):\n",
    "        print('Slice size: ', slice_size)\n",
    "        limit = limits[s]\n",
    "        estimator = svm.slice_estimator(estimator=inner_estimator, slice_size=slice_size, force_unbiased=True, adjust_outer_bias=True, seed=0)\n",
    "        fits = svm.count_hyperparameter_optimization_fits(estimator, search_space=search_space, vectors=vectors, labels=labels, folds=4, filter=True, limit=limit, mode='bayes', seed=0, print_info=True)\n",
    "        fit_count += fits\n",
    "fit_count2 = fit_count\n",
    "print(fit_count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice size:  44\n",
      "Slice size:  59\n",
      "Slice size:  88\n",
      "Slice size:  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:29<00:00, 29.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# file_name = 'Polaris/pgp-broccatelli.parquet'\n",
    "# file_name = 'Polaris/bbb-martins.parquet'\n",
    "file_name = 'Polaris/ncats-solubility.parquet'\n",
    "\n",
    "matrix, matrix_labels, names = svm.read_parquet(file_name)\n",
    "\n",
    "slice_sizes = [int(177/n) for n in [4,3,2,1]]\n",
    "N = 1\n",
    "limits = [34,26,18,12]\n",
    "fit_count = 0\n",
    "\n",
    "for i in tqdm(range(0,N)):\n",
    "    vectors, labels, test_vectors, test_labels = svm.prepare_data_sets(matrix, matrix_labels, train_percentage=50, positive_negative_ratio=None, max_train_size=None, min_train_size=None, seed=i, normalize_data=True, print_info=False)\n",
    "    default_gamma = 1 / (vectors.shape[1] * vectors.var())\n",
    "    # param_grid = {'base': [2,10], 'num_encoding': [1,2,3,4],'kernel': [n*default_gamma for n in [0.2,0.4]], 'penalty': [0,1]}\n",
    "    search_space = {\n",
    "        'base': sks.Categorical([2,10]),\n",
    "        'num_encoding': sks.Integer(1,4),\n",
    "        'kernel': sks.Real(0.05 * default_gamma, 2 * default_gamma, prior='log-uniform'),\n",
    "        'penalty': sks.Real(0,6),\n",
    "    }\n",
    "    # search_sizes = svm.estimate_search_space_size(svm.split_dwave_search_space(search_space, slice_size=None, max_qubo_dim=177), samples_per_real_interval=5)\n",
    "    # limit = round(sum([svm.samples_per_size(search_size) for search_size in search_sizes]))\n",
    "    inner_estimator = svm.qSVM_estimator(solver='SA', adjust_bias=False)\n",
    "    for s, slice_size in enumerate(slice_sizes):\n",
    "        print('Slice size: ', slice_size)\n",
    "        limit = limits[s]\n",
    "        estimator = svm.slice_estimator(estimator=inner_estimator, slice_size=slice_size, force_unbiased=False, adjust_outer_bias=True, seed=0)\n",
    "        fits = svm.count_hyperparameter_optimization_fits(estimator, search_space=search_space, vectors=vectors, labels=labels, folds=4, filter=True, limit=limit, mode='bayes', seed=0, print_info=True)\n",
    "        fit_count += fits\n",
    "\n",
    "fit_count3 = fit_count\n",
    "print(fit_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time left in minutes:  3.682943609999967\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "print('Total Time left in minutes: ', 3*60 + 2 + 31.68  / 60 - ((N - 15) * fit_count1 + (N - 15) * fit_count2 + (N - 6) * fit_count3) * qpu_access_time_250 / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.12it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "fit_count4 = 0\n",
    "start = 0\n",
    "N = 1\n",
    "# higher shots tests:\n",
    "file_name = 'Polaris/pgp-broccatelli.parquet'\n",
    "matrix, matrix_labels, names = svm.read_parquet(file_name)\n",
    "for i in tqdm(range(start,N)):\n",
    "    vectors, labels, test_vectors, test_labels = svm.prepare_data_sets(matrix, matrix_labels, train_percentage=50, positive_negative_ratio=None, max_train_size=None, min_train_size=None, seed=i, normalize_data=True, print_info=False)\n",
    "    for slice_size in slice_sizes:\n",
    "        slices, slice_labels, counts = svm.slice_training_data(vectors, labels, slice_size=slice_size, force_unbiased=False, print_info=False, seed=0)\n",
    "        fit_count4 += len(slices)\n",
    "\n",
    "fit_count5 = 0\n",
    "file_name = 'Polaris/bbb-martins.parquet'\n",
    "matrix, matrix_labels, names = svm.read_parquet(file_name)\n",
    "for i in tqdm(range(start,N)):\n",
    "    vectors, labels, test_vectors, test_labels = svm.prepare_data_sets(matrix, matrix_labels, train_percentage=50, positive_negative_ratio=None, max_train_size=None, min_train_size=None, seed=i, normalize_data=True, print_info=False)\n",
    "    for slice_size in slice_sizes:\n",
    "        slices, slice_labels, counts = svm.slice_training_data(vectors, labels, slice_size=slice_size, force_unbiased=True, print_info=False, seed=0)\n",
    "        fit_count5 += len(slices)\n",
    "\n",
    "fit_count6 = 0\n",
    "file_name = 'Polaris/ncats-solubility.parquet'\n",
    "matrix, matrix_labels, names = svm.read_parquet(file_name)\n",
    "for i in tqdm(range(start,N)):\n",
    "    vectors, labels, test_vectors, test_labels = svm.prepare_data_sets(matrix, matrix_labels, train_percentage=50, positive_negative_ratio=None, max_train_size=None, min_train_size=None, seed=i, normalize_data=True, print_info=False)\n",
    "    for slice_size in slice_sizes:\n",
    "        slices, slice_labels, counts = svm.slice_training_data(vectors, labels, slice_size=slice_size, force_unbiased=False, print_info=False, seed=0)\n",
    "        fit_count6 += len(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 32.93333333333333\n"
     ]
    }
   ],
   "source": [
    "N = 18\n",
    "print(N, 32 + 56/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.418258466666666"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fit_count4 + fit_count5 + fit_count6) * 100 * qpu_access_time_50 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice size:  44\n",
      "Slice size:  59\n",
      "Slice size:  88\n",
      "Slice size:  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:42<00:00, 102.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253.67344464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = 'Polaris/pgp-broccatelli.parquet'\n",
    "# file_name = 'Polaris/bbb-martins.parquet'\n",
    "# file_name = 'Polaris/ncats-solubility.parquet'\n",
    "\n",
    "matrix, matrix_labels, names = svm.read_parquet(file_name)\n",
    "\n",
    "slice_sizes = [int(177/n) for n in [4,3,2,1]]\n",
    "N = 1\n",
    "limits = [34,26,18,12]\n",
    "fit_count7 = 0\n",
    "\n",
    "for i in tqdm(range(0,N)):\n",
    "    vectors, labels, test_vectors, test_labels = svm.prepare_data_sets(matrix, matrix_labels, train_percentage=50, positive_negative_ratio=None, max_train_size=None, min_train_size=None, seed=i, normalize_data=True, print_info=False)\n",
    "    default_gamma = 1 / (vectors.shape[1] * vectors.var())\n",
    "    # param_grid = {'base': [2,10], 'num_encoding': [1,2,3,4],'kernel': [n*default_gamma for n in [0.2,0.4]], 'penalty': [0,1]}\n",
    "    search_space = {\n",
    "        'base': sks.Categorical([2,10]),\n",
    "        'kernel': sks.Real(0.05 * default_gamma, 2 * default_gamma, prior='log-uniform'),\n",
    "        'penalty': sks.Real(0,6),\n",
    "    }\n",
    "    # search_sizes = svm.estimate_search_space_size(svm.split_dwave_search_space(search_space, slice_size=None, max_qubo_dim=177), samples_per_real_interval=5)\n",
    "    # limit = round(sum([svm.samples_per_size(search_size) for search_size in search_sizes]))\n",
    "    inner_estimator = svm.qSVM_estimator(solver='SA', num_encoding=1, adjust_bias=False)\n",
    "    for s, slice_size in enumerate(slice_sizes):\n",
    "        print('Slice size: ', slice_size)\n",
    "        limit = limits[s]\n",
    "        estimator = svm.slice_estimator(estimator=inner_estimator, slice_size=slice_size, force_unbiased=True, adjust_outer_bias=True, seed=0)\n",
    "        fits = svm.count_hyperparameter_optimization_fits(estimator, search_space=search_space, vectors=vectors, labels=labels, folds=4, filter=True, limit=limit, mode='bayes', seed=0, print_info=True)\n",
    "        fit_count7 += fits\n",
    "\n",
    "print(fit_count7 * qpu_access_time_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.139453720000002\n"
     ]
    }
   ],
   "source": [
    "print(fit_count7 * qpu_access_time_250 / 60 * 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1218 (650 positive, 568 negative)\n",
      "0 vectors removed due to inconsistently labelled degeneracies\n",
      "Training data size: 609 (330 positive, 279 negative)\n",
      "Slice size:  44\n",
      "Slice size:  59\n",
      "Slice size:  88\n",
      "Slice size:  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:48<03:13, 48.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1218 (650 positive, 568 negative)\n",
      "0 vectors removed due to inconsistently labelled degeneracies\n",
      "Training data size: 609 (332 positive, 277 negative)\n",
      "Slice size:  44\n",
      "Slice size:  59\n",
      "Slice size:  88\n",
      "Slice size:  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:38<02:28, 49.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1218 (650 positive, 568 negative)\n",
      "0 vectors removed due to inconsistently labelled degeneracies\n",
      "Training data size: 609 (327 positive, 282 negative)\n",
      "Slice size:  44\n",
      "Slice size:  59\n",
      "Slice size:  88\n",
      "Slice size:  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [02:32<01:42, 51.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1218 (650 positive, 568 negative)\n",
      "0 vectors removed due to inconsistently labelled degeneracies\n",
      "Training data size: 609 (315 positive, 294 negative)\n",
      "Slice size:  44\n",
      "Slice size:  59\n",
      "Slice size:  88\n",
      "Slice size:  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [03:23<00:51, 51.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1218 (650 positive, 568 negative)\n",
      "0 vectors removed due to inconsistently labelled degeneracies\n",
      "Training data size: 609 (328 positive, 281 negative)\n",
      "Slice size:  44\n",
      "Slice size:  59\n",
      "Slice size:  88\n",
      "Slice size:  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:19<00:00, 51.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.528326279999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import SVM_scripts as svm\n",
    "import sklearn as skl\n",
    "import skopt\n",
    "import skopt.space as sks\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "data_name = 'pgp-broccatelli'\n",
    "# data_name = 'bbb-martins'\n",
    "# data_name = 'ncats-solubility'\n",
    "\n",
    "run_name = 'QA'\n",
    "\n",
    "file_name = 'Polaris/' + data_name + '.parquet'\n",
    "matrix, matrix_labels, names = svm.read_parquet(file_name)\n",
    "\n",
    "slice_sizes = [int(177/n) for n in [4,3,2,1]]\n",
    "limits = [34,26,18,12]\n",
    "N = 5\n",
    "\n",
    "start = 0\n",
    "test_values_list, best_params, param_tables, cv_results = ({slice_size: [] for slice_size in slice_sizes} for _ in range(4))\n",
    "data_sets, search_spaces = [], []\n",
    "fit_count = 0\n",
    "for i in tqdm(range(start,N)):\n",
    "    vectors, labels, test_vectors, test_labels = svm.prepare_data_sets(matrix, matrix_labels, train_percentage=50, positive_negative_ratio=None, max_train_size=None, min_train_size=None, seed=i, normalize_data=True, print_info=True)\n",
    "    default_gamma = 1 / (vectors.shape[1] * vectors.var())\n",
    "    search_space = {\n",
    "        'base': sks.Categorical([2,10]),\n",
    "        'num_encoding': sks.Integer(1,4),\n",
    "        'kernel': sks.Real(0.05 * default_gamma, 2 * default_gamma, prior='log-uniform'),\n",
    "        'penalty': sks.Real(0,6),\n",
    "    }\n",
    "    search_spaces += [search_space]\n",
    "    data_sets += [(vectors, labels, test_vectors, test_labels)]\n",
    "    inner_estimator = svm.qSVM_estimator(solver=('SA', 250), adjust_bias=False)\n",
    "    for s, slice_size in enumerate(slice_sizes):\n",
    "        print('Slice size: ', slice_size)\n",
    "        estimator = svm.slice_estimator(estimator=inner_estimator, slice_size=slice_size, force_unbiased=True, adjust_outer_bias=True, seed=0)\n",
    "        limit = limits[s]\n",
    "        # f, param_table, opt, optimal_params = svm.hyperparameter_optimization(estimator=estimator, search_space=search_space, vectors=vectors, labels=labels, folds=4, mode='bayes', limit=limit, filter=True, print_info='q', seed=0)\n",
    "        fits = svm.count_hyperparameter_optimization_fits(estimator, search_space=search_space, vectors=vectors, labels=labels, folds=4, filter=True, limit=limit, mode='bayes', seed=0, print_info=True)\n",
    "        fit_count += fits\n",
    "print(fit_count * qpu_access_time_250 / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.638340386666666"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 + 10/60 - 20.528326279999998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
